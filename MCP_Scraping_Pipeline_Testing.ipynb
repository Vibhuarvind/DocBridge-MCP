{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dcc6da3",
   "metadata": {},
   "source": [
    "## MCP based Web-Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca472b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.client\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bac128e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# query = \"TDENGINE DB CONNECTION PYTHON\"\n",
    "load_dotenv() # to load all api keys from .env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8e5c19",
   "metadata": {},
   "source": [
    "### Testing the web search functionality in serper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c922ef9d",
   "metadata": {},
   "source": [
    "Serper is a third-party Google Search API for developers which gives structured, real-time Google search results in a fast and affordable way. It allows developers to integrate Google search data into their applications designed for getting organic data for AI Agents, Knowledge Graphs or pull data from internet etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1ffb804",
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_Search(query: str) -> dict | None:     #type hints\n",
    "    conn = http.client.HTTPSConnection(\"google.serper.dev\")\n",
    "    payload = json.dumps({\n",
    "    \"q\": query,\n",
    "    \"num\": 2\n",
    "    })\n",
    "\n",
    "    api_key = os.getenv(\"SERPER_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"‚ùå SERPER_API_KEY not found in .env file\")\n",
    "\n",
    "    headers = {\n",
    "    'X-API-KEY': str(api_key),\n",
    "    'Content-Type': 'application/json'\n",
    "    }\n",
    "\n",
    "    conn.request(\"POST\", \"/search\", payload, headers)\n",
    "    res = conn.getresponse()\n",
    "    data = res.read()\n",
    "    # print(data.decode(\"utf-8\"))\n",
    "    result = json.loads(data.decode(\"utf-8\"))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d57586b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"searchParameters\":{\"q\":\"Chroma DB\",\"type\":\"search\",\"num\":2,\"engine\":\"google\"},\"organic\":[{\"title\":\"Chroma\",\"link\":\"https://www.trychroma.com/\",\"snippet\":\"Chroma is the open-source search and retrieval database for AI applications. ... Getting started is as easy as pip install... pip install chromadb ¬∑ Full ...\",\"position\":1},{\"title\":\"chroma-core/chroma: Open-source search and retrieval ...\",\"link\":\"https://github.com/chroma-core/chroma\",\"snippet\":\"Chroma - the open-source embedding database. The fastest way to build Python or JavaScript LLM apps with memory!\",\"position\":2}],\"credits\":1}\n"
     ]
    }
   ],
   "source": [
    "web_Search(query=\"Chroma DB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63f34edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'searchParameters': {'q': 'langchain with QDrant', 'type': 'search', 'num': 2, 'engine': 'google'}, 'organic': [{'title': 'Qdrant | ü¶úÔ∏èüîó LangChain', 'link': 'https://python.langchain.com/docs/integrations/vectorstores/qdrant/', 'snippet': 'Qdrant (read: quadrant ) is a vector similarity search engine. It provides a production-ready service with a convenient API to store, search, and manage ...', 'position': 1}, {'title': 'Langchain', 'link': 'https://qdrant.tech/documentation/frameworks/langchain/', 'snippet': 'Langchain is a library that makes developing Large Language Model-based applications much easier. It unifies the interfaces to different libraries.', 'position': 2}], 'credits': 1}\n"
     ]
    }
   ],
   "source": [
    "msg = web_Search(query=\"langchain with QDrant\")\n",
    "print(msg) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb30014d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': 'Qdrant | ü¶úÔ∏èüîó LangChain', 'link': 'https://python.langchain.com/docs/integrations/vectorstores/qdrant/', 'snippet': 'Qdrant (read: quadrant ) is a vector similarity search engine. It provides a production-ready service with a convenient API to store, search, and manage ...', 'position': 1}, {'title': 'Langchain', 'link': 'https://qdrant.tech/documentation/frameworks/langchain/', 'snippet': 'Langchain is a library that makes developing Large Language Model-based applications much easier. It unifies the interfaces to different libraries.', 'position': 2}]\n"
     ]
    }
   ],
   "source": [
    "print(msg[\"organic\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641a2a0e",
   "metadata": {},
   "source": [
    "### Making the search asyncronous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f26120b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fdf56da",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m     26\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChroma DB vs Pinecone\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 27\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweb_Search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(resp)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#web_Search(query=\"Chroma DB\") # awaited error \u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\runners.py:33\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m coroutines\u001b[38;5;241m.\u001b[39miscoroutine(main):\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma coroutine was expected, got \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(main))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "SERPER_URL = \"https://google.serper.dev/search\"\n",
    "\n",
    "async def web_Search(query: str) -> dict | None:     #type hints\n",
    "    payload = json.dumps({\n",
    "    \"q\": query,\n",
    "    \"num\": 2\n",
    "    })\n",
    "\n",
    "    api_key = os.getenv(\"SERPER_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"‚ùå SERPER_API_KEY not found in .env file\")\n",
    "\n",
    "    headers = {\n",
    "    'X-API-KEY': str(api_key),\n",
    "    'Content-Type': 'application/json'\n",
    "    }\n",
    "\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        response = await client.post(\n",
    "            SERPER_URL, headers=headers,\n",
    "            data=payload, timeout=30.0\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    \n",
    "query = \"Chroma DB vs Pinecone\"\n",
    "resp = asyncio.run(web_Search(query))\n",
    "print(resp)\n",
    "#web_Search(query=\"Chroma DB\") # awaited error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a24636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating ai web scraping tool\n",
    "# step 1: search web\n",
    "\n",
    "import http.client\n",
    "import json\n",
    "import os\n",
    "import httpx\n",
    "import asyncio\n",
    "from utils import clean_html_to_txt\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# query = \"TDENGINE DB CONNECTION PYTHON\"\n",
    "load_dotenv()\n",
    "\n",
    "SERPER_URL = \"https://google.serper.dev/search\"\n",
    "# query = \"Chroma DB with Pinecone\"\n",
    "\n",
    "async def web_Search(query: str) -> dict | None:     #type hints\n",
    "    payload = json.dumps({\n",
    "    \"q\": query,\n",
    "    \"num\": 2\n",
    "    })\n",
    "\n",
    "    api_key = os.getenv(\"SERPER_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"‚ùå SERPER_API_KEY not found in .env file\")\n",
    "\n",
    "    headers = {\n",
    "    'X-API-KEY': str(api_key),\n",
    "    'Content-Type': 'application/json'\n",
    "    }\n",
    "\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        response = await client.post(\n",
    "            SERPER_URL, headers=headers,\n",
    "            data=payload, timeout=30.0\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    \n",
    "\n",
    "res = asyncio.run(web_Search(query=\"Chroma DB\"))\n",
    "print(res)\n",
    "\n",
    "\n",
    "# step 2: go and open official documentation only not any other due to security purposes\n",
    "\n",
    "async def fetch_url(url: str) -> str:\n",
    "\n",
    "    #client\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        # hit request to url\n",
    "        response = await client.post(url,timeout=30.0) #response from internet\n",
    "\n",
    "        #parse n clean Data\n",
    "        cleaned_reponse = clean_html_to_txt(response.text)\n",
    "\n",
    "        #return cleaned data\n",
    "        return cleaned_reponse\n",
    "\n",
    "# step 3: after reading doc debug code accordingly - write tool function based on above supporting functions\n",
    "\n",
    "docs_urls = [\n",
    "        \"chroma-db\":\"https://chromadb.readthedocs.io/en/latest/\",\n",
    "        \"Pinecone\":\"https://docs.pinecone.io/docs/quickstart\",\n",
    "        \"Langchain\":\"https://python.langchain.com/docs/\",\n",
    "        \"uv\":\"https://www.uvicorn.org/\",\n",
    "        \"Openai\":\"https://platform.openai.com/docs\",\n",
    "        \"llama-index\":\"https://gpt-index.readthedocs.io/en/latest/\",\n",
    "        \"FastAPI\":\"https://fastapi.tiangolo.com/\"\n",
    "]\n",
    "\n",
    "async def get_docs(query :str, library: str):\n",
    "\n",
    "    \"\"\" \n",
    "    Search to get official latest documentation content based on query and library name \n",
    "    supports langchain, openai, chromadb, pinecone, uvicorn, fastapi, llama-index.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query to find e.g. \"Build a REST API endpoint to upload files\".\n",
    "        library (str): The library to be searched is e.g. \"fastapi\".\n",
    "    \n",
    "    Returns:\n",
    "        str: Summarized content from official documentation.\n",
    "    \"\"\"\n",
    "\n",
    "    # we dont wanrt to scrape general docs, but official docs reponses should be received from web search\n",
    "    # if tool not found in docs then create general responses\n",
    "\n",
    "    if library not in docs_urls:\n",
    "        raise ValueError(f\"‚ùå Library {library}  documentation URL not found\")\n",
    "\n",
    "    # strategy to get what we want => https://python.langchain.com/docs/ chromadb connection\n",
    "    query = f\"site:{docs_urls[library]} {query}\"\n",
    "\n",
    "    results = await web_Search(query=query)\n",
    "\n",
    "    # if we dont get any results\n",
    "    if len(results['organic']) == 0:\n",
    "        return f\"‚ùå No results found for query: {query}\"\n",
    "    \n",
    "    # looping thru results in organic key and fetching url content\n",
    "    for r in results['organic']:\n",
    "\n",
    "        link = r.get(\"link\",\"\") # getting link from each organic result\n",
    "\n",
    "        text_part = []\n",
    "\n",
    "        raw = await fetch_url(link)\n",
    "\n",
    "        # we want to see authentic info, so need to see from which url we are getting content\n",
    "        if raw:\n",
    "            labeled = f\"Source: {link}\\n{raw}\"\n",
    "            print(\"Source :\",link)\n",
    "            text_part.append(labeled)\n",
    "        \n",
    "    return \"\\n\\n\".join(text_part)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
