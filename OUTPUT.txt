uv run mcp_client.py
Available Tools: ['get_docs']
Tool Response: meta=None content=[TextContent(type='text', text='Source: https://python.langchain.com/docs/integrations/vectorstores/chroma/\nChroma\nThis notebook covers how to 
get started with the Chroma\nvector store.\nChroma is a AI-native open-source vector database focused on developer productivity and happiness. Chroma is licensed under Apache 2.0. View the full docs of\nChroma\nat this page, and find the API reference for the LangChain integration at this page.\nChroma Cloud powers serverless vector and full-text search. It\'s extremely fast, cost-effective, scalable and painless. Create a DB and try it out in under 30 seconds with $5 of free credits.\nSetup\nTo access Chroma\nvector stores you\'ll need to install the langchain-chroma\nintegration package.\npip install -qU "langchain-chroma>=0.1.2"\nCredentials\nYou can use the Chroma\nvector store without any credentials, simply installing the package above is enough!\nIf you are a Chroma Cloud user, set your CHROMA_TENANT\n, CHROMA_DATABASE\n, and CHROMA_API_KEY\nenvironment variables.\nWhen 
you install the chromadb\npackage you also get access to the Chroma CLI, which can set these for you. First, login via the CLI, and then use the connect\ncommand:\nchroma db connect [db_name] --env-file\nIf you want to get best in-class automated tracing of your model calls you can also set your LangSmith API key by uncommenting below:\n# os.environ["LANGSMITH_API_KEY"] = getpass.getpass("Enter your LangSmith API key: ")\n# os.environ["LANGSMITH_TRACING"] = "true"\nInitialization\nBasic Initialization\nBelow is a basic initialization, including the use of a directory to save the data locally.\npip install -qU langchain-openai\nimport getpass\nimport os\nif not os.environ.get("OPENAI_API_KEY"):\nos.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")\nfrom langchain_openai import OpenAIEmbeddings\nembeddings = OpenAIEmbeddings(model="text-embedding-3-large")\nRunning Locally (In-Memory)\nYou can get a Chroma server running in memory by simply instantiating a Chroma\ninstance with a collection name and your embeddings provider:\nfrom langchain_chroma import Chroma\nvector_store = Chroma(\ncollection_name="example_collection",\nembedding_function=embeddings,\n)\nIf you don\'t need data persistence, this is a 
great option for experimenting while building your AI application with Langchain.\nRunning Locally (with Data Persistence)\nYou can provide the persist_directory\nargument to save your data across multiple runs of your program:\nfrom langchain_chroma import Chroma\nvector_store = Chroma(\ncollection_name="example_collection",\nembedding_function=embeddings,\npersist_directory="./chroma_langchain_db",\n)\nConnecting to a Chroma Server\nIf you have a Chroma server running locally, or you have deployed one yourself, you can connect to it by providing the host\nargument.\nFor example, you can start a Chroma server running locally with chroma run\n, and then connect it with host=\'localhost\'\n:\nfrom langchain_chroma import Chroma\nvector_store = Chroma(\ncollection_name="example_collection",\nembedding_function=embeddings,\nhost="localhost",\n)\nFor other deployments you can use the port\n, ssl\n, and headers\narguments to customize your connection.\nChroma Cloud\nChroma Cloud users can also build with Langchain. Provide your Chroma\ninstance with your Chroma Cloud API key, tenant, and DB name:\nfrom langchain_chroma import Chroma\nvector_store = Chroma(\ncollection_name="example_collection",\nembedding_function=embeddings,\nchroma_cloud_api_key=os.getenv("CHROMA_API_KEY"),\ntenant=os.getenv("CHROMA_TENANT"),\ndatabase=os.getenv("CHROMA_DATABASE"),\n)\nInitialization from client\nYou can also initialize 
from a Chroma\nclient, which is particularly useful if you want easier access to the underlying database.\nRunning Locally (In-Memory)\nimport chromadb\nclient = chromadb.Client()\nRunning Locally (with Data Persistence)\nimport chromadb\nclient = chromadb.PersistentClient(path="./chroma_langchain_db")\nConnecting to a Chroma Server\nFor example, if you 
are running a Chroma server locally (using chroma run\n):\nimport chromadb\nclient = chromadb.HttpClient(host="localhost", port=8000, ssl=False)\nChroma Cloud\nAfter setting your CHROMA_API_KEY\n, CHROMA_TENANT\n, and CHROMA_DATABASE\n, you can simply instantiate:\nimport chromadb\nclient = chromadb.CloudClient()\nAccess your Chroma DB\ncollection = client.get_or_create_collection("collection_name")\ncollection.add(ids=["1", "2", "3"], documents=["a", "b", "c"])\nCreate a Chroma Vectorstore\nvector_store_from_client = Chroma(\nclient=client,\ncollection_name="collection_name",\nembedding_function=embeddings,\n)\nManage vector store\nOnce you have created your vector store, we can interact with it by adding and deleting different items.\nAdd items to vector store\nWe can add items to our vector store by using the add_documents\nfunction.\nfrom uuid import uuid4\nfrom langchain_core.documents import Document\ndocument_1 = Document(\npage_content="I had chocolate chip pancakes and scrambled eggs for breakfast this morning.",\nmetadata={"source": "tweet"},\nid=1,\n)\ndocument_2 = Document(\npage_content="The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.",\nmetadata={"source": "news"},\nid=2,\n)\ndocument_3 = Document(\npage_content="Building an exciting new project with LangChain - come check it out!",\nmetadata={"source": "tweet"},\nid=3,\n)\ndocument_4 = Document(\npage_content="Robbers broke into the city bank and stole $1 million in cash.",\nmetadata={"source": "news"},\nid=4,\n)\ndocument_5 = Document(\npage_content="Wow! That was an amazing movie. I can\'t wait to see it again.",\nmetadata={"source": "tweet"},\nid=5,\n)\ndocument_6 = Document(\npage_content="Is the new iPhone worth the price? Read this review to find out.",\nmetadata={"source": "website"},\nid=6,\n)\ndocument_7 = Document(\npage_content="The top 10 soccer players in the world right now.",\nmetadata={"source": "website"},\nid=7,\n)\ndocument_8 = Document(\npage_content="LangGraph is the best framework for building stateful, agentic applications!",\nmetadata={"source": "tweet"},\nid=8,\n)\ndocument_9 = Document(\npage_content="The stock market is down 500 points today due to fears of a recession.",\nmetadata={"source": "news"},\nid=9,\n)\ndocument_10 = Document(\npage_content="I have a bad feeling I am going to get deleted :(",\nmetadata={"source": "tweet"},\nid=10,\n)\ndocuments = [\ndocument_1,\ndocument_2,\ndocument_3,\ndocument_4,\ndocument_5,\ndocument_6,\ndocument_7,\ndocument_8,\ndocument_9,\ndocument_10,\n]\nuuids = [str(uuid4()) for _ in range(len(documents))]\nvector_store.add_documents(documents=documents, ids=uuids)\nUpdate items in vector store\nNow that we have added documents to our vector store, we can update existing documents by using the update_documents\nfunction.\nupdated_document_1 = Document(\npage_content="I had chocolate chip pancakes and fried eggs for breakfast this morning.",\nmetadata={"source": "tweet"},\nid=1,\n)\nupdated_document_2 = Document(\npage_content="The weather forecast for tomorrow is sunny and warm, with a high of 82 degrees.",\nmetadata={"source": "news"},\nid=2,\n)\nvector_store.update_document(document_id=uuids[0], document=updated_document_1)\n# You can also update multiple documents at once\nvector_store.update_documents(\nids=uuids[:2], documents=[updated_document_1, updated_document_2]\n)\nDelete items from vector store\nWe can also delete items from our vector store as follows:\nvector_store.delete(ids=uuids[-1])\nFork a vector store\nForking lets you create a new Chroma\nvector store from an existing one instantly, using copy-on-write under the hood. This means that your new Chroma\nstore is identical to the origin, but any modifications to it will not affect the origin, and vice-versa.\nForks are great for any use case that benefits from data versioning. You can learn more about forking in the Chroma docs.\nNote: Forking is only avaiable on Chroma\ninstances with a Chroma Cloud connection.\nforked_store = vector_store.fork(new_name="my_forked_collection")\nupdated_document_2 = Document(\npage_content="The weather forecast for tomorrow is extrmeley hot, with a high of 100 degrees.",\nmetadata={"source": "news"},\nid=2,\n)\n# Update does not 
affect \'vector_store\'\nforked_store.update(ids=["2"], documents=[updated_document_2])\nQuery vector store\nOnce your vector store has been created and the relevant documents have been added you will most likely wish to query it during the running of your chain or agent.\nQuery directly\nSimilarity search\nPerforming a simple similarity search can be done as follows:\nresults = vector_store.similarity_search(\n"LangChain provides abstractions to make working with LLMs easy",\nk=2,\nfilter={"source": "tweet"},\n)\nfor res in results:\nprint(f"* {res.page_content} [{res.metadata}]")\nSimilarity search with score\nIf you want to execute a similarity search and receive the corresponding scores you can run:\nresults = vector_store.similarity_search_with_score(\n"Will it be hot tomorrow?", k=1, filter={"source": "news"}\n)\nfor res, score in results:\nprint(f"* [SIM={score:3f}] {res.page_content} [{res.metadata}]")\nSearch by vector\nYou can also search by vector:\nresults = vector_store.similarity_search_by_vector(\nembedding=embeddings.embed_query("I love green eggs and ham!"), k=1\n)\nfor doc in results:\nprint(f"* {doc.page_content} [{doc.metadata}]")\nOther search methods\nThere are a variety of other search methods that are not covered in this notebook. For a full list of the search abilities available for Chroma\ncheck out the API reference.\nQuery by turning into retriever\nYou can also transform the vector store into a retriever for easier usage in your chains. For more information on the different search types and kwargs you can pass, please visit the API reference here.\nretriever = vector_store.as_retriever(\nsearch_type="mmr", search_kwargs={"k": 1, "fetch_k": 5}\n)\nretriever.invoke("Stealing from the bank is a crime", filter={"source": "news"})\nUsage for retrieval-augmented generation\nFor guides on how to use this vector store for retrieval-augmented generation (RAG), see the following sections:\nAPI reference\nFor detailed documentation of all Chroma\nvector store features and configurations head to the API reference: https://python.langchain.com/api_reference/chroma/vectorstores/langchain_chroma.vectorstores.Chroma.html\nRelated\n- Vector store conceptual guide\n- Vector store how-to guides\n\nSource: https://python.langchain.com/docs/integrations/retrievers/self_query/chroma_self_query/\nChroma\nChroma is a vector database for building AI applications with embeddings.\nIn the notebook, we\'ll demo the SelfQueryRetriever\nwrapped around a Chroma\nvector store.\nCreating a Chroma vector store\nFirst we\'ll want to create a Chroma vector store and seed it with some data. We\'ve created a small demo set of documents that contain summaries of movies.\nNote: The self-query retriever requires you to have lark\ninstalled (pip install lark\n). We also need the langchain-chroma\npackage.\n%pip install --upgrade --quiet lark\n%pip install --upgrade --quiet langchain-chroma\nWe want to use OpenAIEmbeddings\nso we have to get the OpenAI API Key.\nimport getpass\nimport 
os\nif "OPENAI_API_KEY" not in os.environ:\nos.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")\nOpenAI API Key: ········\nfrom langchain_chroma import Chroma\nfrom langchain_core.documents import Document\nfrom langchain_openai import OpenAIEmbeddings\nembeddings = OpenAIEmbeddings()\ndocs = [\nDocument(\npage_content="A bunch of scientists bring back dinosaurs and mayhem breaks loose",\nmetadata={"year": 1993, "rating": 7.7, "genre": "science fiction"},\n),\nDocument(\npage_content="Leo DiCaprio gets lost in a dream within a dream within a dream within a ...",\nmetadata={"year": 2010, "director": "Christopher Nolan", "rating": 8.2},\n),\nDocument(\npage_content="A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea",\nmetadata={"year": 2006, "director": "Satoshi Kon", "rating": 8.6},\n),\nDocument(\npage_content="A bunch of normal-sized women are supremely wholesome and some men pine after them",\nmetadata={"year": 2019, "director": "Greta Gerwig", "rating": 8.3},\n),\nDocument(\npage_content="Toys come alive and have a blast doing so",\nmetadata={"year": 1995, "genre": "animated"},\n),\nDocument(\npage_content="Three men walk into the Zone, three men walk out of the Zone",\nmetadata={\n"year": 1979,\n"director": "Andrei Tarkovsky",\n"genre": "science fiction",\n"rating": 9.9,\n},\n),\n]\nvectorstore = Chroma.from_documents(docs, embeddings)\nUsing embedded DuckDB without persistence: data will be transient\nCreating our self-querying retriever\nNow we can instantiate our retriever. To do this we\'ll need to provide some information upfront about the metadata fields that our documents support and a short description of the document contents.\nfrom langchain.chains.query_constructor.schema import AttributeInfo\nfrom langchain.retrievers.self_query.base import SelfQueryRetriever\nfrom langchain_openai import OpenAI\nmetadata_field_info = [\nAttributeInfo(\nname="genre",\ndescription="The genre of the movie",\ntype="string or list[string]",\n),\nAttributeInfo(\nname="year",\ndescription="The year the movie was released",\ntype="integer",\n),\nAttributeInfo(\nname="director",\ndescription="The name of the movie director",\ntype="string",\n),\nAttributeInfo(\nname="rating", description="A 1-10 rating 
for the movie", type="float"\n),\n]\ndocument_content_description = "Brief summary of a movie"\nllm = OpenAI(temperature=0)\nretriever = SelfQueryRetriever.from_llm(\nllm, vectorstore, document_content_description, metadata_field_info, verbose=True\n)\nTesting it out\nAnd now we can try actually using our retriever!\n# This example only specifies a relevant query\nretriever.invoke("What are some movies about dinosaurs")\nquery=\'dinosaur\' filter=None\n[Document(page_content=\'A bunch of scientists bring back dinosaurs and mayhem breaks loose\', metadata={\'year\': 1993, \'rating\': 7.7, \'genre\': \'science fiction\'}),\nDocument(page_content=\'Toys come alive and have a blast doing so\', metadata={\'year\': 1995, \'genre\': \'animated\'}),\nDocument(page_content=\'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\', metadata={\'year\': 2006, \'director\': \'Satoshi Kon\', \'rating\': 8.6}),\nDocument(page_content=\'Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\', metadata={\'year\': 2010, \'director\': \'Christopher Nolan\', \'rating\': 8.2})]\n# This example only specifies a filter\nretriever.invoke("I want to watch a movie rated higher than 8.5")\nquery=\' \' filter=Comparison(comparator=<Comparator.GT: \'gt\'>, attribute=\'rating\', value=8.5)\n[Document(page_content=\'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\', metadata={\'year\': 2006, \'director\': \'Satoshi Kon\', \'rating\': 8.6}),\nDocument(page_content=\'Three men walk into the Zone, three men walk out of the Zone\', metadata={\'year\': 1979, \'rating\': 9.9, \'director\': \'Andrei Tarkovsky\', \'genre\': \'science fiction\'})]\n# This example specifies a query and a filter\nretriever.invoke("Has Greta Gerwig directed any movies about women")\nquery=\'women\' filter=Comparison(comparator=<Comparator.EQ: \'eq\'>, attribute=\'director\', value=\'Greta Gerwig\')\n[Document(page_content=\'A bunch of normal-sized women are supremely wholesome and some men pine after them\', metadata={\'year\': 2019, \'director\': \'Greta Gerwig\', \'rating\': 8.3})]\n# This example specifies a composite filter\nretriever.invoke("What\'s a highly rated (above 8.5) science fiction film?")\nquery=\' \' filter=Operation(operator=<Operator.AND: \'and\'>, arguments=[Comparison(comparator=<Comparator.EQ: \'eq\'>, attribute=\'genre\', value=\'science fiction\'), Comparison(comparator=<Comparator.GT: \'gt\'>, attribute=\'rating\', value=8.5)])\n[Document(page_content=\'Three men walk into the Zone, three men walk out of 
the Zone\', metadata={\'year\': 1979, \'rating\': 9.9, \'director\': \'Andrei Tarkovsky\', \'genre\': \'science fiction\'})]\n# This example specifies a query and composite filter\nretriever.invoke(\n"What\'s a movie after 1990 but before 2005 that\'s all about toys, and preferably is animated"\n)\nquery=\'toys\' filter=Operation(operator=<Operator.AND: 
\'and\'>, arguments=[Comparison(comparator=<Comparator.GT: \'gt\'>, attribute=\'year\', value=1990), Comparison(comparator=<Comparator.LT: \'lt\'>, attribute=\'year\', value=2005), Comparison(comparator=<Comparator.EQ: \'eq\'>, attribute=\'genre\', value=\'animated\')])\n[Document(page_content=\'Toys come alive and have a blast doing so\', metadata={\'year\': 1995, \'genre\': \'animated\'})]\nFilter k\nWe can also use the self query retriever to specify k\n: the number of documents to fetch.\nWe can do this by passing enable_limit=True\nto the constructor.\nretriever = SelfQueryRetriever.from_llm(\nllm,\nvectorstore,\ndocument_content_description,\nmetadata_field_info,\nenable_limit=True,\nverbose=True,\n)\n# This example only specifies a relevant query\nretriever.invoke("what are two movies about dinosaurs")\nquery=\'dinosaur\' filter=None\n[Document(page_content=\'A bunch of 
scientists bring back dinosaurs and mayhem breaks loose\', metadata={\'year\': 1993, \'rating\': 7.7, \'genre\': \'science fiction\'}),\nDocument(page_content=\'Toys come alive and have a blast doing so\', metadata={\'year\': 1995, \'genre\': \'animated\'}),\nDocument(page_content=\'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\', metadata={\'year\': 2006, \'director\': \'Satoshi Kon\', \'rating\': 8.6}),\nDocument(page_content=\'Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\', metadata={\'year\': 2010, \'director\': \'Christopher Nolan\', \'rating\': 8.2})]', annotations=None, meta=None)] structuredContent=None isError=False